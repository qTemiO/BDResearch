{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9833d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48cd63b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Victim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAttacker\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    The base class of all attackers. Each attacker has a poisoner and a trainer.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m        metrics (`List[str]`, optional): the metrics to evaluate.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     15\u001b[0m             poisoner: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbase\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     20\u001b[0m     ):\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36mAttacker\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoisoner \u001b[38;5;241m=\u001b[39m load_poisoner(poisoner)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoison_trainer \u001b[38;5;241m=\u001b[39m load_trainer(\u001b[38;5;28mdict\u001b[39m(poisoner, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoison_method\u001b[39m\u001b[38;5;124m\"\u001b[39m:poisoner[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]}))\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattack\u001b[39m(\u001b[38;5;28mself\u001b[39m, victim: \u001b[43mVictim\u001b[49m, data: List, config: Optional[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, defender: Optional[Defender] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    Attack the victim model with the attacker.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     poison_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoison(victim, data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Victim' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "\n",
    "class Attacker(object):\n",
    "    \"\"\"\n",
    "    The base class of all attackers. Each attacker has a poisoner and a trainer.\n",
    "\n",
    "    Args:\n",
    "        poisoner (:obj:`dict`, optional): the config of poisoner.\n",
    "        train (:obj:`dict`, optional): the config of poison trainer.\n",
    "        metrics (`List[str]`, optional): the metrics to evaluate.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            poisoner: Optional[dict] = {\"name\": \"base\"},\n",
    "            train: Optional[dict] = {\"name\": \"base\"},\n",
    "            metrics: Optional[List[str]] = [\"accuracy\"],\n",
    "            sample_metrics: Optional[List[str]] = [],\n",
    "            **kwargs\n",
    "    ):\n",
    "        self.metrics = metrics\n",
    "        self.sample_metrics = sample_metrics\n",
    "        self.poisoner_config = poisoner\n",
    "        self.trainer_config = train\n",
    "        self.poisoner = load_poisoner(poisoner)\n",
    "        self.poison_trainer = load_trainer(dict(poisoner, **train, **{\"poison_method\":poisoner[\"name\"]}))\n",
    "\n",
    "    def attack(self, victim: Victim, data: List, config: Optional[dict] = None, defender: Optional[Defender] = None):\n",
    "        \"\"\"\n",
    "        Attack the victim model with the attacker.\n",
    "\n",
    "        Args:\n",
    "            victim (:obj:`Victim`): the victim to attack.\n",
    "            data (:obj:`List`): the dataset to attack.\n",
    "            defender (:obj:`Defender`, optional): the defender.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Victim`: the attacked model.\n",
    "\n",
    "        \"\"\"\n",
    "        poison_dataset = self.poison(victim, data, \"train\")\n",
    "\n",
    "        if defender is not None and defender.pre is True:\n",
    "            # pre tune defense\n",
    "            poison_dataset[\"train\"] = defender.correct(poison_data=poison_dataset['train'])\n",
    "\n",
    "        backdoored_model = self.train(victim, poison_dataset)\n",
    "        return backdoored_model\n",
    "\n",
    "    def poison(self, victim: Victim, dataset: List, mode: str):\n",
    "        \"\"\"\n",
    "        Default poisoning function.\n",
    "\n",
    "        Args:\n",
    "            victim (:obj:`Victim`): the victim to attack.\n",
    "            dataset (:obj:`List`): the dataset to attack.\n",
    "            mode (:obj:`str`): the mode of poisoning. \n",
    "        \n",
    "        Returns:\n",
    "            :obj:`List`: the poisoned dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.poisoner(dataset, mode)\n",
    "\n",
    "    def train(self, victim, dataset: List):\n",
    "        \"\"\"\n",
    "        Use ``poison_trainer`` to attack the victim model.\n",
    "        default training: normal training\n",
    "\n",
    "        Args:\n",
    "            victim (:obj:`Victim`): the victim to attack.\n",
    "            dataset (:obj:`List`): the dataset to attack.\n",
    "    \n",
    "        Returns:\n",
    "            :obj:`Victim`: the attacked model.\n",
    "        \"\"\"\n",
    "        return self.poison_trainer.train(victim, dataset, self.metrics)\n",
    "\n",
    "    def eval(self, victim: Victim, dataset: List, defender: Optional[Defender] = None):\n",
    "        \"\"\"\n",
    "        Default evaluation function (ASR and CACC) for the attacker.\n",
    "            \n",
    "        Args:\n",
    "            victim (:obj:`Victim`): the victim to attack.\n",
    "            dataset (:obj:`List`): the dataset to attack.\n",
    "            defender (:obj:`Defender`, optional): the defender.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`dict`: the evaluation results.\n",
    "        \"\"\"\n",
    "        poison_dataset = self.poison(victim, dataset, \"eval\")\n",
    "        if defender is not None and defender.pre is False:\n",
    "            \n",
    "            if defender.correction:\n",
    "                poison_dataset[\"test-clean\"] = defender.correct(model=victim, clean_data=dataset, poison_data=poison_dataset[\"test-clean\"])\n",
    "                poison_dataset[\"test-poison\"] = defender.correct(model=victim, clean_data=dataset, poison_data=poison_dataset[\"test-poison\"])\n",
    "            else:\n",
    "                # post tune defense\n",
    "                detect_poison_dataset = self.poison(victim, dataset, \"detect\")\n",
    "                detection_score, preds = defender.eval_detect(model=victim, clean_data=dataset, poison_data=detect_poison_dataset)\n",
    "                \n",
    "                clean_length = len(poison_dataset[\"test-clean\"])\n",
    "                num_classes = len(set([data[1] for data in poison_dataset[\"test-clean\"]]))\n",
    "                preds_clean, preds_poison = preds[:clean_length], preds[clean_length:]\n",
    "                poison_dataset[\"test-clean\"] = [(data[0], num_classes, 0) if pred == 1 else (data[0], data[1], 0) for pred, data in zip(preds_clean, poison_dataset[\"test-clean\"])]\n",
    "                poison_dataset[\"test-poison\"] = [(data[0], num_classes, 0) if pred == 1 else (data[0], data[1], 0) for pred, data in zip(preds_poison, poison_dataset[\"test-poison\"])]\n",
    "\n",
    "\n",
    "        poison_dataloader = wrap_dataset(poison_dataset, self.trainer_config[\"batch_size\"])\n",
    "        \n",
    "        results = evaluate_classification(victim, poison_dataloader, self.metrics)\n",
    "\n",
    "        sample_metrics = self.eval_poison_sample(victim, dataset, self.sample_metrics)\n",
    "\n",
    "        return dict(results[0], **sample_metrics)\n",
    "\n",
    "\n",
    "    def eval_poison_sample(self, victim, dataset: List, eval_metrics=[]):\n",
    "        \"\"\"\n",
    "        Evaluation function for the poison samples (PPL, Grammar Error, and USE).\n",
    "\n",
    "        Args:\n",
    "            victim (:obj:`Victim`): the victim to attack.\n",
    "            dataset (:obj:`List`): the dataset to attack.\n",
    "            eval_metrics (:obj:`List`): the metrics for samples. \n",
    "        \n",
    "        Returns:\n",
    "            :obj:`List`: the poisoned dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        evaluator = Evaluator()\n",
    "        sample_metrics = {\"ppl\": np.nan, \"grammar\": np.nan, \"use\": np.nan}\n",
    "        \n",
    "        poison_dataset = self.poison(victim, dataset, \"eval\")\n",
    "        clean_test = self.poisoner.get_non_target(poison_dataset[\"test-clean\"])\n",
    "        poison_test = poison_dataset[\"test-poison\"]\n",
    "\n",
    "        for metric in eval_metrics:\n",
    "            if metric not in ['ppl', 'grammar', 'use']:\n",
    "                logger.info(\"  Invalid Eval Metric, return  \")\n",
    "            measure = 0\n",
    "            if metric == 'ppl':\n",
    "                measure = evaluator.evaluate_ppl([item[0] for item in clean_test], [item[0] for item in poison_test])\n",
    "            if metric == 'grammar':\n",
    "                measure = evaluator.evaluate_grammar([item[0] for item in clean_test], [item[0] for item in poison_test])\n",
    "            if metric == 'use':\n",
    "                measure = evaluator.evaluate_use([item[0] for item in clean_test], [item[0] for item in poison_test])\n",
    "            logger.info(\"  Eval Metric: {} =  {}\".format(metric, measure))\n",
    "            sample_metrics[metric] = measure\n",
    "        \n",
    "        return sample_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310a5b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b3678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f542d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a75e5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb4b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b08ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727320c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
